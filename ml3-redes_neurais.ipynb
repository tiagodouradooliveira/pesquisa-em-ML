{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "GMrO1_D3iIFl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "O2m8YWP1iHN4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a7a3a83d-89e6-4db7-f0c9-4a0a6e413970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "import pickle\n",
        "import gzip\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datasets_path='/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tópicos estudados do livro *Neural Networks and Deep Learning*:\n",
        "\n",
        "*   Using neural nets to recognize handwritten digits\n",
        "*   How the backpropagation algorithm works\n",
        "*   Improving the way neural networks learn\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OTgzML5viKzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Expansão dos modelos lineares para as redes neurais\n",
        "\n",
        "A unidade básica das redes neurais são os *neurônios* artificiais, levemente inspirados nos neurônios do cérebro. Neurônios artificiais, de modo similar aos perceptrons, aceitam entradas $x_1, x_2, \\dots, x_n$ e, por meio de pesos $w_1, w_2, \\dots, w_n$ para cada entrada entrada e um viés $b$, gera uma saída $0$ ou $1$. Porém, o problema de perceptrons simples para redes neurais é que fica difícil realizar as pequenas mudanças necessárias em pesos e vieses, já que elas podem gerar grandes mudanças nas saídas. Assim, usamos os neurônios sigmoides para tarefas do tipo, já que eles garantem pequenas mudanças de saída para pequenas mudanças de pesos e viés. Um neurônio sigmoide é similar perceptron, mas, em vez de gerar a saída diretamente de $w\\cdot x + b$, ele gera um saída com a função $\\sigma(z)$, chamada de função sigmoide, de modo que\n",
        "\\begin{align*}\n",
        "\\sigma(z)&=\\sigma\\biggl(\\frac{1}{1+e^{-z}}\\biggr)\\\\\n",
        "&=\\frac{1}{1+\\exp(-\\sum_j w_jx_j - b)},\n",
        "\\end{align*}\n",
        "com saída entre $0$ e $1$. Podemos interpretar isso como uma probabilidade, assim como o caso da regressão logística. Note também que $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))=\\frac{e^{-z}}{(1+e^{-z})^2}$\n",
        "\n",
        "A função sigmoide está representada abaixo."
      ],
      "metadata": {
        "id": "ievfM_FIiqHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xx = np.linspace(-10, 10)\n",
        "plt.title(\"Função sigmoide\")\n",
        "plt.xlim(-10, 10)\n",
        "plt.ylim(-0.5, 1.5)\n",
        "plt.plot(xx, 1 / (1 + np.exp(-xx)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "WeL3wBbslYsF",
        "outputId": "1b5413a3-e662-4f86-973a-ca900661bf90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79480ee62a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAGzCAYAAAAlqLNlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOb0lEQVR4nO3deVhUZf8G8HtmgGGdAWQZUEQFA1dcIbRFkwQ101Y1yyXTFrMUM+V9U1xKynzTMl9tdWk138o2xRS1fiZuuKUiiYEIMiggDCDMwMzz+wOZZmQRlWFY7s91zcWc5zznzPeZkzN3Z84iEUIIEBEREREAQGrtAoiIiIiaEoYjIiIiIhMMR0REREQmGI6IiIiITDAcEREREZlgOCIiIiIywXBEREREZILhiIiIiMgEwxERERGRCYYjIrKoV155BS4uLpg4cSLy8/PRtWtXHDt2rFFrWL9+PSQSCdLT0xv1dW9Whw4dMGnSpBv2ay7jIWquGI6ImrmqL8qaHvPmzbNqbcXFxVizZg0WL16MU6dOwcPDA87OzujZs6dV6yIiqouNtQsgooaxePFidOzY0ayte/fuVqqmkr29PU6fPg1/f3/MmjULFy9ehEqlglTauP9f9tRTT2Hs2LGQy+WN+ro3KyUlpdHfGyKqjuGIqIUYNmwY+vXrZ+0yzNjY2MDf39847evra5U6ZDIZZDKZVV77ZjT18EbUWvB/UYhaAYlEgoULF1Zrv/4Yl6qf6P744w9ER0fD09MTTk5OeOihh3D58uVqy2/btg333nsvXFxcoFAo0L9/f3z55ZfG+Xv27MGjjz6K9u3bQy6Xw8/PD7NmzUJpaWm1de3atQt33303nJyc4OrqilGjRiE5Oble41u1ahW6desGR0dHuLm5oV+/fmZ11HSMjsFgwMKFC+Hr6wtHR0cMHjwYp0+frvU92bt3L1566SV4enrC1dUVzz77LHQ6HQoKCjBhwgS4ubnBzc0Nr776KoQQZvWVlJRg9uzZ8PPzg1wuR1BQEJYvX16tX03HHJ06dQr33XcfHBwc0K5dO7z++uswGAw1vg/btm0zvocuLi4YMWIETp06Va/3kIj+wT1HRC1EYWEhcnNzzdo8PDxuaV0zZsyAm5sbYmNjkZ6ejpUrV+LFF1/Epk2bjH3Wr1+Pp59+Gt26dUNMTAxcXV1x9OhRxMfH44knngAAfPPNNygtLcULL7wAd3d3HDx4EKtWrUJmZiY2b95sXNfOnTsxbNgwdOrUCQsXLkRpaSlWrVqFgQMH4siRI+jQoUOttX700Ud46aWX8Oijj+Lll19GWVkZTpw4gQMHDhjrqElMTAyWLVuGkSNHIjIyEsePH0dkZCTKyspqfU9UKhUWLVqE/fv348MPP4Srqyv27duH9u3bY+nSpdi6dSvefvttdO/eHRMmTAAACCHw4IMPYvfu3ZgyZQp69eqF7du3Y86cOcjKysKKFStqrVGtVmPw4MGoqKjAvHnz4OTkhA8//BAODg7V+n722WeYOHEiIiMj8dZbb+Hq1atYs2YN7rrrLhw9erTO95CIriOIqFlbt26dAFDjowoAERsbW21Zf39/MXHixGrrioiIEAaDwdg+a9YsIZPJREFBgRBCiIKCAuHi4iLCwsJEaWmp2TpNlyspKan2mnFxcUIikYjz588b23r16iW8vLxEXl6ese348eNCKpWKCRMm1Dn+UaNGiW7dutXZp2pcaWlpQggh1Gq1sLGxEaNHjzbrt3DhQgGgxvckMjLSbGzh4eFCIpGI5557zthWUVEh2rVrJ+69915j25YtWwQA8frrr5u91qOPPiokEolITU01tl2/PWbOnCkAiAMHDhjbLl26JJRKpdl4ioqKhKurq5g6darZa6jVaqFUKqu1E1Hd+LMaUQuxevVq7Nixw+xxq6ZNmwaJRGKcvvvuu6HX63H+/HkAwI4dO1BUVIR58+bB3t7ebFnT5RwdHY3PS0pKkJubiwEDBkAIgaNHjwIAsrOzcezYMUyaNAnu7u7G/j179sT999+PrVu31lmrq6srMjMzcejQoXqPLyEhARUVFXjhhRfM2mfMmFHrMlOmTDEbW1hYGIQQmDJlirFNJpOhX79++Pvvv41tW7duhUwmw0svvWS2vtmzZ0MIgW3bttX6mlu3bsWdd96J0NBQY5unpyfGjx9v1m/Hjh0oKCjAuHHjkJuba3zIZDKEhYVh9+7dtb4GEVXHn9WIWojQ0NAGOyC7ffv2ZtNubm4AgCtXrgAAzp07B+DGZ8NlZGRgwYIF+PHHH43LViksLAQAY+AKCgqqtnyXLl2wfft2lJSUwMnJqcbXmDt3Lnbu3InQ0FAEBgZi6NCheOKJJzBw4MBa66p6zcDAQLN2d3d341ivd/17olQqAQB+fn7V2k3Hev78efj6+sLFxaXa2Exrqa3OsLCwau3Xv1dnz54FANx33301rkehUNT6GkRUHcMRUSum1+trbK/tzC5x3QHEN1r3/fffj/z8fMydOxfBwcFwcnJCVlYWJk2aVOtBxTerS5cuSElJwc8//4z4+Hh8++23+O9//4sFCxZg0aJFDfIaQO3vSU3tN/M+NYSq9/Kzzz6DSqWqNt/Ghh/1RDeD/2KIWgE3NzcUFBSYtel0OmRnZ9/S+gICAgAAJ0+erLb3pcqff/6Jv/76Cxs2bDAenAyg2s99Vaf6p6SkVFvHmTNn4OHhUeteoypOTk4YM2YMxowZA51Oh4cffhhvvPEGYmJiqv3sZ/qaqampZteGysvLq7aH63b5+/tj586dKCoqMtt7dObMGbNaalu2aq+Qqevfq6rt4eXlhYiIiIYom6hV4zFHRK1AQEAAfv/9d7O2Dz/8sNY9RzcydOhQuLi4IC4urtrZXVV7Tar2qJjuRRFC4N133zXr7+Pjg169emHDhg1mAe7kyZP49ddfMXz48DprycvLM5u2s7ND165dIYRAeXl5jcsMGTIENjY2WLNmjVn7+++/X+dr3Yrhw4dDr9dXW/eKFSsgkUgwbNiwOpfdv38/Dh48aGy7fPkyvvjiC7N+kZGRUCgUWLp0aY1jrukyDERUO+45ImoFnnnmGTz33HN45JFHcP/99+P48ePYvn37LZ/qr1AosGLFCjzzzDPo378/nnjiCbi5ueH48eO4evUqNmzYgODgYAQEBOCVV15BVlYWFAoFvv322xr3zLz99tsYNmwYwsPDMWXKFOOp/EqlssbrM5kaOnQoVCoVBg4cCG9vbyQnJ+P999/HiBEjqh3nU8Xb2xsvv/wy/vOf/+DBBx9EVFQUjh8/jm3btsHDw8PswOvbNXLkSAwePBj//ve/kZ6ejpCQEPz666/44YcfMHPmTONen5q8+uqr+OyzzxAVFYWXX37ZeCq/v78/Tpw4YeynUCiwZs0aPPXUU+jTpw/Gjh0LT09PZGRk4JdffsHAgQMtEvyIWiyrnSdHRA2i6lTzQ4cO1dpHr9eLuXPnCg8PD+Ho6CgiIyNFampqrafyX7+u3bt3CwBi9+7dZu0//vijGDBggPHSAaGhoeKrr74yzj99+rSIiIgQzs7OwsPDQ0ydOlUcP35cABDr1q0zW9fOnTvFwIEDhYODg1AoFGLkyJHi9OnTNxz/Bx98IO655x7Rpk0bIZfLRUBAgJgzZ44oLCysNq6qU9+FqDztfv78+UKlUgkHBwdx3333ieTkZNGmTRuz0/Nre09iY2MFAHH58mWz9okTJwonJyeztqKiIjFr1izh6+srbG1tRefOncXbb79tdmkAIaqfyi+EECdOnBD33nuvsLe3F23bthVLliwRn3zySbXxCFG5nSIjI4VSqRT29vYiICBATJo0SRw+fPiG7yMR/UMiRCMfOUhELU5RURG6d++OpKSkW94b1RQUFBTAzc0Nr7/+Ov79739buxwishIec0REt83FxQV9+vTBjz/+aO1S6q2mW5isXLkSADBo0KDGLYaImhQec0REt2X58uVwcXHB/v37MXjwYGuXU2+bNm3C+vXrMXz4cDg7O2Pv3r346quvMHTo0DqvkURELR/DERHdlp9//hmJiYno3bt3nfcya2p69uwJGxsbLFu2DBqNxniQ9uuvv27t0ojIyiz6s9rvv/+OkSNHwtfXFxKJBFu2bKmz/549eyCRSKo91Gq1Wb/Vq1ejQ4cOsLe3R1hYmNlprkTUuPbs2QOtVov9+/c3q+ON+vTpg507dyI3Nxc6nQ4XLlzAypUr4ezsbO3SiMjKLBqOSkpKEBISgtWrV9/UcikpKcjOzjY+vLy8jPM2bdqE6OhoxMbG4siRIwgJCUFkZCQuXbrU0OUTERFRK9RoZ6tJJBJ8//33GD16dK199uzZg8GDB+PKlStwdXWtsU9YWBj69+9vvGaHwWCAn58fZsyYgXnz5lmgciIiImpNmuQxR7169YJWq0X37t2xcOFC48GROp0OSUlJiImJMfaVSqWIiIhAYmJirevTarXQarXGaYPBgPz8fLRp06ZBL/ZGREREliOEQFFREXx9fSGVWu7HryYVjnx8fLB27Vr069cPWq0WH3/8MQYNGoQDBw6gT58+yM3NhV6vh7e3t9ly3t7exvsU1SQuLq5Bb0BJRERE1nPhwgW0a9fOYutvUuEoKCgIQUFBxukBAwbg3LlzWLFiBT777LNbXm9MTAyio6ON04WFhWjfvj0uXLgAhUJxWzUTERFR49BoNPDz86v11kANpUmFo5qEhoZi7969AAAPDw/IZDLk5OSY9cnJyYFKpap1HXK5HHK5vFq7QqFgOCIiImpmLH1ITJO/QvaxY8fg4+MDoPJu23379kVCQoJxvsFgQEJCAsLDw61VIhEREbUgFt1zVFxcjNTUVON0Wloajh07Bnd3d7Rv3x4xMTHIysrCxo0bAVReur9jx47o1q0bysrK8PHHH2PXrl349ddfjeuIjo7GxIkT0a9fP4SGhmLlypUoKSnB5MmTLTkUIiIiaiUsGo4OHz5sdjuBquN+Jk6ciPXr1yM7OxsZGRnG+TqdDrNnz0ZWVhYcHR3Rs2dP7Ny502wdY8aMweXLl7FgwQKo1Wr06tUL8fHx1Q7SJiIiIroVjXado6ZEo9FAqVSisLCQxxwRERE1E431/d3kjzkiIiIiakwMR0REREQmGI6IiIiITDAcEREREZlgOCIiIiIywXBEREREZILhiIiIiMgEwxERERGRCYYjIiIiIhMMR0REREQmGI6IiIiITDAcEREREZlgOCIiIiIywXBEREREZILhiIiIiMgEwxERERGRCYYjIiIiIhMMR0REREQmGI6IiIiITDAcEREREZlgOCIiIiIywXBEREREZILhiIiIiMgEwxERERGRCYYjIiIiIhMMR0REREQmGI6IiIiITDAcEREREZlgOCIiIiIywXBEREREZILhiIiIiMgEwxERERGRCYYjIiIiIhMMR0REREQmGI6IiIiITFg0HP3+++8YOXIkfH19IZFIsGXLljr7f/fdd7j//vvh6ekJhUKB8PBwbN++3azPwoULIZFIzB7BwcEWHAURERG1JhYNRyUlJQgJCcHq1avr1f/333/H/fffj61btyIpKQmDBw/GyJEjcfToUbN+3bp1Q3Z2tvGxd+9eS5RPRERErZCNJVc+bNgwDBs2rN79V65caTa9dOlS/PDDD/jpp5/Qu3dvY7uNjQ1UKlVDlUlERERk1KSPOTIYDCgqKoK7u7tZ+9mzZ+Hr64tOnTph/PjxyMjIqHM9Wq0WGo3G7EFERERUkyYdjpYvX47i4mI8/vjjxrawsDCsX78e8fHxWLNmDdLS0nD33XejqKio1vXExcVBqVQaH35+fo1RPhERETVDEiGEaJQXkkjw/fffY/To0fXq/+WXX2Lq1Kn44YcfEBERUWu/goIC+Pv745133sGUKVNq7KPVaqHVao3TGo0Gfn5+KCwshEKhuKlxEBERkXVoNBoolUqLf39b9JijW/X111/jmWeewebNm+sMRgDg6uqKO+64A6mpqbX2kcvlkMvlDV0mERERtUBN7me1r776CpMnT8ZXX32FESNG3LB/cXExzp07Bx8fn0aojoiIiFo6i+45Ki4uNtujk5aWhmPHjsHd3R3t27dHTEwMsrKysHHjRgCVP6VNnDgR7777LsLCwqBWqwEADg4OUCqVAIBXXnkFI0eOhL+/Py5evIjY2FjIZDKMGzfOkkMhIiKiVsKie44OHz6M3r17G0/Dj46ORu/evbFgwQIAQHZ2ttmZZh9++CEqKiowffp0+Pj4GB8vv/yysU9mZibGjRuHoKAgPP7442jTpg32798PT09PSw6FiIiIWolGOyC7KWmsA7qIiIio4TTW93eTO+aIiIiIyJoYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmbBoOPr9998xcuRI+Pr6QiKRYMuWLTdcZs+ePejTpw/kcjkCAwOxfv36an1Wr16NDh06wN7eHmFhYTh48GDDF09EREStkkXDUUlJCUJCQrB69ep69U9LS8OIESMwePBgHDt2DDNnzsQzzzyD7du3G/ts2rQJ0dHRiI2NxZEjRxASEoLIyEhcunTJUsMgIiKiVkQihBCN8kISCb7//nuMHj261j5z587FL7/8gpMnTxrbxo4di4KCAsTHxwMAwsLC0L9/f7z//vsAAIPBAD8/P8yYMQPz5s2rVy0ajQZKpRKFhYVQKBS3PigiIiJqNI31/W1jsTXfgsTERERERJi1RUZGYubMmQAAnU6HpKQkxMTEGOdLpVJEREQgMTGx1vVqtVpotVrjtEajadjCiYioyRFCQFthQFm5HmXlBpSW61FWrkdpuR7lFQbo9AaU6w3QVYjK5xXXpvUG6CoMKNcLVOgNqDAI6A3i2t/KdtPpqvl6g4BBCBgMgF4IGAwCevFPe+VfAAKV/UTltLhWa9Wyhmv7LAxCQJjMF2Z9AYFrbcJ8zJV9KufD+BzG5zBpr5qq2k9iurfk+l0npvtSzGZd36+WZWrpXuNr1UavLalfx9vUpMKRWq2Gt7e3WZu3tzc0Gg1KS0tx5coV6PX6GvucOXOm1vXGxcVh0aJFFqmZiIgalhACRdoK5BfrkFeiQ8FVHYq1FSgqq3wUa8tRXFaBIm0FissqUKy99iirMAtA2gpDvb90qXkwlBsa5XWaVDiylJiYGERHRxunNRoN/Pz8rFgREVHrU643QF1YhswrpcgqKIW6sBR5JTrkl+iQdy0I5ZdocaWkHDp9w34J2soksLeRQW4rg72tFHIbKWxlUtjZSGEnq3xue+25nY2kclomha1MAhupFDKpBDZSCWSya3+lUthem5ZJJJBJKx9SiQRSaVVb5SElVfOlUgkkwLV+lfOkksrnUokEEmMbIEHlX5g8l1zrI5UAwLX+uNYOQCKpHKvk2jzUo8182ny+aatp2z/9JNXajNNm/a+fWzNJPboVaTTotrJeq7stTSocqVQq5OTkmLXl5ORAoVDAwcEBMpkMMpmsxj4qlarW9crlcsjlcovUTERElYQQuFhYhr8vFyPrSqkxBGVeuYqsK6VQa8oqf1aqJ0c7Gdyd7ODuZAcXexs4y23gYm977W/ltPO1vwp7WzjJbeBgK4ODnRRyGxnsbWVwsJPB3kYKGxmvXNMSaGwqGuV1mlQ4Cg8Px9atW83aduzYgfDwcACAnZ0d+vbti4SEBOOB3QaDAQkJCXjxxRcbu1wiolbrqq4CKeoinFEXITlbgzPZRUhWa1BUVveXl52NFG1dHdDW1QE+Snu0cZajzbUA5O5sBw8nOdyd7dDGyQ72trJGGg2ROYuGo+LiYqSmphqn09LScOzYMbi7u6N9+/aIiYlBVlYWNm7cCAB47rnn8P777+PVV1/F008/jV27duGbb77BL7/8YlxHdHQ0Jk6ciH79+iE0NBQrV65ESUkJJk+ebMmhEBG1WiXaChxMz8eJC4U4o9bgjLoI6XklNR7PYyuToEMbJ7Rzc0BbNwe0dXU0Pm/n6gAPZzmk0vr9zEJkLRYNR4cPH8bgwYON01XH/UycOBHr169HdnY2MjIyjPM7duyIX375BbNmzcK7776Ldu3a4eOPP0ZkZKSxz5gxY3D58mUsWLAAarUavXr1Qnx8fLWDtImI6NZU6A04nlmIP1JzsTc1F0czrqBcXz0JebrIEaxyQVcfBYJ9XBCsUiDA0xl2NvwJi5q3RrvOUVPC6xwREf1DCIFzl4ux92wu9qbm4cDfeSjSmv881tbVAWEd3dHVV4FgVWUY8nDmsZzUuFrldY6IiKhxCCFwMC0f3x7JxG9/XUaORms239XRFgMDPDAw0AMDA9ugvbuj2dlJRC0ZwxERUSuSoynD/5IysfnwBaTnXTW2y22kCO3ojoGBHrgr0ANdfRQ8NohaLYYjIqIWTldhwK4zl/DN4QvYk3LJeDq9k50MD/T0xYO9fNHX341nhxFdw3BERNRCnc0pwjeHL+C7I1nIK9EZ2/t3cMNj/fwwoocPnOT8GiC6Hv9VEBG1MPtSc/GfHX8h6fwVY5unixyP9GmHx/u1QydPZytWR9T0MRwREbUQZ9QavLntDPakXAYA2EgluC/YC4/388OgIE9eJZqonhiOiIiauezCUrzz61/435FMCFEZip680x8vDA6Al4u9tcsjanYYjoiImilNWTk++O0cPtmbhrJrdysf0cMHcyKD0MHDycrVETVfDEdERM2MrsKALw+cx3u7UpF/7UDr/h3c8K/hXdC7vZuVqyNq/hiOiIiaCSEEtv6pxrLtZ3D+2jWKOnk6YV5UMO7v6s2LNBI1EIYjIqJmoKisHPO+/RO//JkNAPBwlmPW/Z0xpp8fD7QmamAMR0RETVxytgYvfHEEabklsJFK8MLgQDx7Tydeo4jIQvgvi4ioCfvm0AXM/+EktBUG+Cjt8f4TfdDXn8cVEVkSwxERURNUqtNj/g8n8b+kTADAoCBPvPN4L7g72Vm5MqKWj+GIiKiJOXe5GNO/OIIz6iJIJcDsoUF4/t4A3giWqJEwHBERNSE/Hb+Ied+eQIlODw9nOd4b1wsDAjysXRZRq8JwRETUBGgr9Hjjl2RsTDwPAAjr6I5V43rDS8ErXBM1NoYjIiIryy4sxXOfJeF4ZiEAYPrgAMyKuIOn6BNZCcMREZEVqQvLMPbD/TifdxWujrZYMaYXBgd5WbssolaN4YiIyEouacrwxEeVwcjP3QFfPnMn/NwdrV0WUavHcEREZAWXi7R44uMD+Du3BG1dHfDV1DvRzo3BiKgp4A/aRESNLL9Ehyc/PoDUS8XwUdozGBE1MQxHRESNqOCqDuM/PoCUnCJ4ucjx1dQ70b4NgxFRU8JwRETUSAqvluPJTw4gOVsDD2c5vpp2Jzp4OFm7LCK6DsMREVEj0JSVY8KnB3AyS4M2Tnb4amoYAjydrV0WEdWA4YiIyMKKtRWY9OlBHM8shJujLb6YGobO3i7WLouIasFwRERkQSXaCkxedxBHMgqgdLDF58+EIVilsHZZRFQHhiMiIgsp1ekxZcMhHEq/Ahd7G3w+JQzdfJXWLouIboDhiIjIAoQQmLXpGPb/nQ9nuQ02Ph2KHu0YjIiaA4YjIiIL+GRvGuJPqWEnk2Ld5P7o3d7N2iURUT0xHBERNbDD6fmI23YGADD/gS7o38HdyhUR0c1gOCIiakC5xVq8+OVR6A0CD4b44sk7/a1dEhHdJIYjIqIGojcIzPz6GNSaMgR6OSPu4R6QSCTWLouIbhLDERFRA3k34Sz2pubCwVaGNeP7wEnOe3sTNUcMR0REDWBPyiWs2nUWABD3cA9e5JGoGWuUcLR69Wp06NAB9vb2CAsLw8GDB2vtO2jQIEgkkmqPESNGGPtMmjSp2vyoqKjGGAoRUTVZBaWYtekYhADGh7XH6N5trV0SEd0Gi+/z3bRpE6Kjo7F27VqEhYVh5cqViIyMREpKCry8vKr1/+6776DT6YzTeXl5CAkJwWOPPWbWLyoqCuvWrTNOy+Vyyw2CiKgWugoDpn9xBFeulqNHWyXmP9DV2iUR0W2y+J6jd955B1OnTsXkyZPRtWtXrF27Fo6Ojvj0009r7O/u7g6VSmV87NixA46OjtXCkVwuN+vn5sZriBBR41u6NRnHLhRAYW+D/47vA3tbmbVLIqLbZNFwpNPpkJSUhIiIiH9eUCpFREQEEhMT67WOTz75BGPHjoWTk5NZ+549e+Dl5YWgoCA8//zzyMvLq3UdWq0WGo3G7EFEdLt+OZGN9fvSAQDvPN4Lfu6O1i2IiBqERcNRbm4u9Ho9vL29zdq9vb2hVqtvuPzBgwdx8uRJPPPMM2btUVFR2LhxIxISEvDWW2/ht99+w7Bhw6DX62tcT1xcHJRKpfHh5+d364MiIgJw7nIxXv3fcQDAc/cGIKKr9w2WIKLmokmfZ/rJJ5+gR48eCA0NNWsfO3as8XmPHj3Qs2dPBAQEYM+ePRgyZEi19cTExCA6Oto4rdFoGJCI6JaV6vR44fMjKNHpEdrRHa8MvcPaJRFRA7LoniMPDw/IZDLk5OSYtefk5EClUtW5bElJCb7++mtMmTLlhq/TqVMneHh4IDU1tcb5crkcCoXC7EFEdKsW/3wKKTlF8HCW4/1xvWEj41VRiFoSi/6LtrOzQ9++fZGQkGBsMxgMSEhIQHh4eJ3Lbt68GVqtFk8++eQNXyczMxN5eXnw8fG57ZqJiOqy71wuvjp4AQDw3rhe8FLYW7kiImpoFv/fnejoaHz00UfYsGEDkpOT8fzzz6OkpASTJ08GAEyYMAExMTHVlvvkk08wevRotGnTxqy9uLgYc+bMwf79+5Geno6EhASMGjUKgYGBiIyMtPRwiKgVKyvX49/fnwQAPHlnewwI8LByRURkCRY/5mjMmDG4fPkyFixYALVajV69eiE+Pt54kHZGRgakUvOMlpKSgr179+LXX3+ttj6ZTIYTJ05gw4YNKCgogK+vL4YOHYolS5bwWkdEZFGrd6ciLbcEXi5yvBoVbO1yiMhCJEIIYe0iGptGo4FSqURhYSGPPyKieklRF2HEe/+HCoPA2if7IKo7f8YnamyN9f3NowiJiG7AYBCI+e4EKgwC93f1RmS3uk8oIaLmjeGIiOgGvjiYgSMZBXCyk2HxqG6QSCTWLomILIjhiIioDjmaMizbdgYA8GpUMHyUDlauiIgsjeGIiKgOC388hSJtBXr5ueLJO/2tXQ4RNQKGIyKiWvx6So1tJ9WwkUoQ93APyKT8OY2oNWA4IiKqQVFZORb8cAoAMPWeTujiwzNbiVoLhiMiohr859e/oNaUwb+NI14e0tna5RBRI2I4IiK6ztGMK9iQmA4AeGN0D9jbyqxbEBE1KoYjIiIT5XoDYr77E0IAD/dpi7s68xYhRK0NwxERkYmP/u9vnFEXwc3RFq+N6GrtcojIChiOiIiuSc8twbs7zwIA5j/QFe5OdlauiIisgeGIiAiAEALzfzgJbYUBdwV64KHeba1dEhFZCcMRERGAXWcu4f/O5sJOJsUbD3XnLUKIWjGGIyJq9cr1BizdmgwAePqujvBv42TliojImhiOiKjV++pgBs5dLkEbJzu8MDjA2uUQkZUxHBFRq1ZYWo4VO/4CAMy8/w4o7G2tXBERWRvDERG1av/dnYorV8sR6OWMcf39rF0OETUBDEdE1Gpl5F3Fuj/SAQD/HtEFNjJ+JBIRwxERtWJvxZ+BTm/A3Z09MOgOT2uXQ0RNBMMREbVKh9Pz8cuf2ZBKKvca8dR9IqrCcERErY7BILDkl8pT98f090OwSmHlioioKWE4IqJW56cTF3H8QgGc7GSYdf8d1i6HiJoYhiMialXKyvVYFp8CAHh+UAC8XOytXBERNTUMR0TUqnz6RxqyCkrhq7THM3d3snY5RNQEMRwRUatxuUiL/+4+BwCYExUEe1uZlSsioqaI4YiIWo0VO/9CsbYCPdspMSqkrbXLIaImiuGIiFqFv3KK8PXBDADAayO6QirlqftEVDOGIyJqFd74JRkGAUR1UyG0o7u1yyGiJozhiIhavN/+uozf/roMW5kE84YFW7scImriGI6IqEXTGwSWXrvg48TwDujg4WTlioioqWM4IqIW7dsjmUjJKYLSwRYz7uts7XKIqBlgOCKiFqusXI8VO/4CALw4OBBKR1srV0REzQHDERG1WBsT05FdWAZfpT2eCve3djlE1EwwHBFRi1R4tRyrr13wMXooL/hIRPXHcERELdKa386hsLQcQd4ueKg3L/hIRPXHcERELU52YSnW/ZEGAHg1KggyXvCRiG5Co4Sj1atXo0OHDrC3t0dYWBgOHjxYa9/169dDIpGYPeztze+aLYTAggUL4OPjAwcHB0RERODs2bOWHgYRNRMrd5yFtsKA0A7uuC/Yy9rlEFEzY/FwtGnTJkRHRyM2NhZHjhxBSEgIIiMjcenSpVqXUSgUyM7ONj7Onz9vNn/ZsmV47733sHbtWhw4cABOTk6IjIxEWVmZpYdDRE3c2ZwibE66AACYOywYEgn3GhHRzbF4OHrnnXcwdepUTJ48GV27dsXatWvh6OiITz/9tNZlJBIJVCqV8eHt7W2cJ4TAypUr8dprr2HUqFHo2bMnNm7ciIsXL2LLli01rk+r1UKj0Zg9iKhlWrY9BQYBRHbzRl9/N2uXQ0TNkEXDkU6nQ1JSEiIiIv55QakUERERSExMrHW54uJi+Pv7w8/PD6NGjcKpU6eM89LS0qBWq83WqVQqERYWVus64+LioFQqjQ8/P78GGB0RNTWH0/Ox43QOpBJgTiRvE0JEt8ai4Sg3Nxd6vd5szw8AeHt7Q61W17hMUFAQPv30U/zwww/4/PPPYTAYMGDAAGRmZgKAcbmbWWdMTAwKCwuNjwsXLtzu0IioiRFC4K34MwCAMf39EOjlbOWKiKi5srF2AdcLDw9HeHi4cXrAgAHo0qULPvjgAyxZsuSW1imXyyGXyxuqRCJqghKSL+FQ+hXY20rx8pA7rF0OETVjFt1z5OHhAZlMhpycHLP2nJwcqFSqeq3D1tYWvXv3RmpqKgAYl7uddRJRy6I3/LPX6OmBHaFS2t9gCSKi2lk0HNnZ2aFv375ISEgwthkMBiQkJJjtHaqLXq/Hn3/+CR8fHwBAx44doVKpzNap0Whw4MCBeq+TiFqWb49k4uylYigdbPHsvQHWLoeImjmL/6wWHR2NiRMnol+/fggNDcXKlStRUlKCyZMnAwAmTJiAtm3bIi4uDgCwePFi3HnnnQgMDERBQQHefvttnD9/Hs888wyAyjPZZs6ciddffx2dO3dGx44dMX/+fPj6+mL06NGWHg4RNTHVbi7rwJvLEtHtsXg4GjNmDC5fvowFCxZArVajV69eiI+PNx5QnZGRAan0nx1YV65cwdSpU6FWq+Hm5oa+ffti37596Nq1q7HPq6++ipKSEkybNg0FBQW46667EB8fX+1ikUTU8m3Yx5vLElHDkgghhLWLaGwajQZKpRKFhYVQKBTWLoeIblHh1XLcvWwXNGUVWP5YCB7t287aJRGRBTXW9zfvrUZEzdaa385BU1bBm8sSUYNiOCKiZuliwT83l507jDeXJaKGw3BERM3S29tTKm8u29Edg4N4c1kiajgMR0TU7By/UIDvj2YBAOaP6MqbyxJRg2I4IqJmRQiBN35JBgA83KcterRTWrkiImppGI6IqFnZfkqNg+n5sLeVYk5kkLXLIaIWiOGIiJoNXYUBcdsqbxMy7e5O8FE6WLkiImqJGI6IqNnYmJiO83lX4eki521CiMhiGI6IqFm4UqLDewlnAQCvDL0DTnKLX+CfiFophiMiahbe23UWmrIKBKtc8GhfP2uXQ0QtGMMRETV5f18uxmeJ5wEAr43oygs+EpFFMRwRUZMXt+0MKgwC9wV74a7OHtYuh4haOIYjImrSEs/lYcfpHMikEvxreLC1yyGiVoDhiIiaLINB4PVfTgMAxoe1R6CXi5UrIqLWgOGIiJqs745m4dRFDVzkNnh5SGdrl0NErQTDERE1SVd1FVi+PQUA8OJ9gWjjLLdyRUTUWjAcEVGT9NHvaVBryuDn7oCJAzpYuxwiakUYjoioycnRlGHtb+cAAHOjgmFvK7NyRUTUmjAcEVGT859fU1Barkef9q4Y0cPH2uUQUSvDcERETcqfmYXYnJQJAHjtga6QSHjBRyJqXAxHRNRkVOgNmPfdCQgBjOrliz7t3axdEhG1QgxHRNRkrPsjHacuaqB0sMVrI7pauxwiaqUYjoioSbiQfxXv7PgLAPDv4V3g6cJT94nIOhiOiMjqhBB4bctJlJbrcWcndzzWr521SyKiVozhiIis7sfjF/HbX5dhZyPF0od68CBsIrIqhiMisqqCqzos/qny/mkzBgeik6ezlSsiotaO4YiIrGrp1mTklejQ2csZz94bYO1yiIgYjojIevady8U3hyuvafTmIz1gZ8OPJCKyPn4SEZFVlJXr8e/vTwIAnryzPfr6u1u5IiKiSgxHRGQVq3enIi23BF4ucrwaFWztcoiIjBiOiKjRpaiLsGZP5Y1lFz3YDQp7WytXRET0D4YjImpUBoNAzHcnUGEQiOjijajuKmuXRERkhuGIiBrVFwczcCSjAE52Miwe1Y3XNCKiJofhiIgajbqwDMu2nQEAzIkMgq+rg5UrIiKqjuGIiBrNwh9PoUhbgRA/VzwV3sHa5RAR1ahRwtHq1avRoUMH2NvbIywsDAcPHqy170cffYS7774bbm5ucHNzQ0RERLX+kyZNgkQiMXtERUVZehhEdBt+On4R8afUsJFK8ObDPSCT8uc0ImqaLB6ONm3ahOjoaMTGxuLIkSMICQlBZGQkLl26VGP/PXv2YNy4cdi9ezcSExPh5+eHoUOHIisry6xfVFQUsrOzjY+vvvrK0kMholt07nIx5n17AgDw/KAAdPFRWLkiIqLaSYQQwpIvEBYWhv79++P9998HABgMBvj5+WHGjBmYN2/eDZfX6/Vwc3PD+++/jwkTJgCo3HNUUFCALVu23FJNGo0GSqUShYWFUCj4IU1kSaU6PUav/gMpOUUI6+iOL54Jg42Mv+gT0c1rrO9vi35C6XQ6JCUlISIi4p8XlEoRERGBxMTEeq3j6tWrKC8vh7u7+dVz9+zZAy8vLwQFBeH5559HXl5erevQarXQaDRmDyKyPCEE/r3lT6TkFMHTRY5VT/RmMCKiJs+in1K5ubnQ6/Xw9vY2a/f29oZara7XOubOnQtfX1+zgBUVFYWNGzciISEBb731Fn777TcMGzYMer2+xnXExcVBqVQaH35+frc+KCKqt02HLuC7I1mQSoD3xvaGl4u9tUsiIrohG2sXUJc333wTX3/9Nfbs2QN7+38+VMeOHWt83qNHD/Ts2RMBAQHYs2cPhgwZUm09MTExiI6ONk5rNBoGJCILO5lViAU/ngIAvBIZhPCANlauiIiofiy658jDwwMymQw5OTlm7Tk5OVCp6r4q7vLly/Hmm2/i119/Rc+ePevs26lTJ3h4eCA1NbXG+XK5HAqFwuxBRJZTWFqO6V8ega7CgCHBXnjungBrl0REVG8WDUd2dnbo27cvEhISjG0GgwEJCQkIDw+vdblly5ZhyZIliI+PR79+/W74OpmZmcjLy4OPj0+D1E1Et04IgTmbj+N83lW0c3PAfx4PgZSn7RNRM2LxIyOjo6Px0UcfYcOGDUhOTsbzzz+PkpISTJ48GQAwYcIExMTEGPu/9dZbmD9/Pj799FN06NABarUaarUaxcXFAIDi4mLMmTMH+/fvR3p6OhISEjBq1CgEBgYiMjLS0sMhohv4+P/S8OvpHNjJpPjv+D5wdbSzdklERDfF4sccjRkzBpcvX8aCBQugVqvRq1cvxMfHGw/SzsjIgFT6T0Zbs2YNdDodHn30UbP1xMbGYuHChZDJZDhx4gQ2bNiAgoIC+Pr6YujQoViyZAnkcrmlh0NEdTiUno834ytvDzJ/ZFf0bOdq3YKIiG6Bxa9z1BTxOkdEDS+3WIsR7/0fcjRajOrli5VjevGmskTUoFrEdY6IqHXQGwRe/voocjRaBHo5Y+lDPRiMiKjZYjgiotv27s6/8EdqHhxsZVgzvg+c5E36KiFERHViOCKi27L9lBrv7aq8jMabj/RAZ28XK1dERHR7GI6I6JbtPnMJL355BADw5J3tMapXWytXRER0+xiOiOiW/P7XZTz7eRLK9QLDe6iwcGQ3a5dERNQgGI6I6Kb9kZqLqRsPQ1dhwNCu3nh3LG8oS0QtBz/NiOim7P87D1M2HIL22q1B3n+iD2wZjIioBeEnGhHV2+H0fDy9/hDKyg249w5P/PfJPrCz4ccIEbUs/FQjono5knEFk9YdwlWdHncFeuCDp/pCbiOzdllERA2O4YiIbuhEZgEmfnIQxdoKhHdqg48m9IO9LYMREbVMDEdEVKeTWYV48uMDKNJWILSDOz6Z1A8OdgxGRNRyMRwRUa2SszV48pMD0JRVoK+/Gz6d3B+Odrz6NRG1bAxHRFSj0xc1GP/xARRcLUeInyvWT+4PZ94WhIhaAX7SEVE1mw9fwPwfTqKs3IAebZXY+HQoXOxtrV0WEVGjYDgiIqOycj0W/HAS3xzOBADcc4cnVo3tDaUDgxERtR4MR0QEAPj7cjFe+OIIzqiLIJUAsyLuwPTBgZBKJdYujYioUTEcERF+OZGNud+eQLG2Ah7OdnhvbG8MCPSwdllERFbBcETUiukqDFi6NRnr96UDAEI7umPVuN7wVthbtzAiIitiOCJqpTKvXMX0L4/i+IUCAMDzgwIw+/47eANZImr1GI6IWqFdZ3Iwa9NxFJaWQ+lgixVjQnBfsLe1yyIiahIYjohakdxiLd7deRaf7T8PAAjxc8XqJ3qjnZujlSsjImo6GI6IWoFSnR6f7P0ba3/7G8XaCgDApAEd8K/hXWBnw5/RiIhMMRwRtWB6g8D/ki7gnR1/IUejBQD0aKtEzPBgDAjg2WhERDVhOCJqgYQQ2J1yCW9uO4O/cooBAO3cHDAnMggje/ry2kVERHVgOCJqYU5kFmDp1mTs/zsfAKB0sMWM+wLxVLg/5DYyK1dHRNT0MRwRtRDnLhdj5c6z+On4RQCAnY0Ukwd2wAv3BkLpyNt/EBHVF8MRUTN2VVeBX05kY/PhTBxMr9xTJJEAD/Vui9lDg9DW1cHKFRIRNT8MR0TNjBACRy8U4JtDF/DT8Yso0ekBAFIJMDjIC7PuvwPd2yqtXCURUfPFcETUTFwu0uL7o5n45nAmUi8VG9v92zji8X5+eLhPW/gouaeIiOh2MRwRNWEFV3XYdy4PW45mYdeZS6gwCACAva0Uw7v74PH+fgjt4M6zz4iIGhDDEVETUlauR9L5K9ibmos/UnPxZ1YhhPhnfoifK8b088MDIT5Q2PMgayIiS2A4IrIivUHg9EWNMQwdSs+HtsJg1qezlzMGBXni0b5+CFK5WKlSIqLWg+GIqJEYDALn86/iTLYGyeoiJGdrcCg9HwVXy836eSvkGBjogbsCPTAw0APeCnsrVUxE1DoxHBFZQGFpOVLURTij1iA5W4Pk7CKkqItQWq6v1tdZboM7O7XBXYFtcFdnDwR4OkMi4TFERETWwnBEdAvKyvXIKihF1pVSZF4pRVbBVePzzCulUGvKalxObiPFHd4u6OLjgmCVAiF+SoS0c4WNjDd/JSJqKholHK1evRpvv/021Go1QkJCsGrVKoSGhtbaf/PmzZg/fz7S09PRuXNnvPXWWxg+fLhxvhACsbGx+Oijj1BQUICBAwdizZo16Ny5c2MMh1oovUHgylUd8op1yCvRIr9Eh/ySyun8ksq2rIIyZF0pRW6x9obra+vqgGCVC4J9XNDFR4FglQId2jgyCBERNXEWD0ebNm1CdHQ01q5di7CwMKxcuRKRkZFISUmBl5dXtf779u3DuHHjEBcXhwceeABffvklRo8ejSNHjqB79+4AgGXLluG9997Dhg0b0LFjR8yfPx+RkZE4ffo07O15fEZrIYSATm9Amc6Asgo9SnX6f/6WG1BaXoFirR7FZRUo1pajuKwCRdqKa9MVKLo2XVRWjislOhSUlpudGXYjjnYytHNzQFtXB7Rzc0Tba8/bujkgwMOZt+wgImqmJELczNfBzQsLC0P//v3x/vvvAwAMBgP8/PwwY8YMzJs3r1r/MWPGoKSkBD///LOx7c4770SvXr2wdu1aCCHg6+uL2bNn45VXXgEAFBYWwtvbG+vXr8fYsWNvWJNGo4FSqcShvy7A2UVRZ9+a3p0a21C9sa531nRe1bLmbVX9xHXT5j2EqHwmhHnfyvbKmabT//QX5m3X2g1V7ULAcG2+wTgtYDD808cgBPRCwGAQ0Bsq+xlE5fOqdoMAKgwCeoOh8q9eoMIgUGEwQG8QqDBOC1ToDSjXG6DTC5RXGKCrmq6o/FuuF9Bday8r16O0XH9TYaa+XB1t0cbJDm2c5HB3soO7sx3aONnB3ckOPkr7yiDk6gBXR1seG0RE1Iiqvr8LCwuhUNT9/X07LLrnSKfTISkpCTExMcY2qVSKiIgIJCYm1rhMYmIioqOjzdoiIyOxZcsWAEBaWhrUajUiIiKM85VKJcLCwpCYmFhjONJqtdBq//kZRKPRAAAe/m8ipHLHWx4fNR0yqQT2NlI42Mkgt5HBwU4GB1sZnOU2cLa3gcu1v9WnbeEst6kMQU52cHO05c9eREStnEXDUW5uLvR6Pby9vc3avb29cebMmRqXUavVNfZXq9XG+VVttfW5XlxcHBYtWlStvY2THWT28huOo747B0y7mS4jgfkKquaZ96/+IsZ+xv6SGpeXSExeQVLZbtomkVQuW7WcVFL5XCK59vxaJ8m1vjKJBNLKhSC91qdqmcrnleuvei6TSiCVSq4tB+NzmVQCiUQCmRSwkUphK5NAJpXCRlo57/ppG5kEdjIp7GyksJVVPuxsJJV/ZVLY2lz7K5PCwVYGezsp7G0rQ5AtAw0RETWQVnG2WkxMjNneKI1GAz8/P/z26mCL7pYjIiKi5sei/7vt4eEBmUyGnJwcs/acnByoVKoal1GpVHX2r/p7M+uUy+VQKBRmDyIiIqKaWDQc2dnZoW/fvkhISDC2GQwGJCQkIDw8vMZlwsPDzfoDwI4dO4z9O3bsCJVKZdZHo9HgwIEDta6TiIiIqL4s/rNadHQ0Jk6ciH79+iE0NBQrV65ESUkJJk+eDACYMGEC2rZti7i4OADAyy+/jHvvvRf/+c9/MGLECHz99dc4fPgwPvzwQwCVx7rMnDkTr7/+Ojp37mw8ld/X1xejR4+29HCIiIiohbN4OBozZgwuX76MBQsWQK1Wo1evXoiPjzceUJ2RkQGp9J8dWAMGDMCXX36J1157Df/617/QuXNnbNmyxXiNIwB49dVXUVJSgmnTpqGgoAB33XUX4uPjeY0jIiIium0Wv85RU9RY10kgIiKihtNY3988/5mIiIjIBMMRERERkQmGIyIiIiITDEdEREREJhiOiIiIiEwwHBERERGZYDgiIiIiMsFwRERERGSC4YiIiIjIBMMRERERkQmGIyIiIiITDEdEREREJhiOiIiIiEwwHBERERGZYDgiIiIiMsFwRERERGSC4YiIiIjIBMMRERERkQmGIyIiIiITDEdEREREJhiOiIiIiEwwHBERERGZYDgiIiIiMsFwRERERGSC4YiIiIjIBMMRERERkQmGIyIiIiITDEdEREREJhiOiIiIiEwwHBERERGZYDgiIiIiMsFwRERERGSC4YiIiIjIBMMRERERkQmGIyIiIiITFgtH+fn5GD9+PBQKBVxdXTFlyhQUFxfX2X/GjBkICgqCg4MD2rdvj5deegmFhYVm/SQSSbXH119/balhEBERUStjY6kVjx8/HtnZ2dixYwfKy8sxefJkTJs2DV9++WWN/S9evIiLFy9i+fLl6Nq1K86fP4/nnnsOFy9exP/+9z+zvuvWrUNUVJRx2tXV1VLDICIiolZGIoQQDb3S5ORkdO3aFYcOHUK/fv0AAPHx8Rg+fDgyMzPh6+tbr/Vs3rwZTz75JEpKSmBjU5njJBIJvv/+e4wePfqW69NoNFAqlSgsLIRCobjl9RAREVHjaazvb4v8rJaYmAhXV1djMAKAiIgISKVSHDhwoN7rqRp8VTCqMn36dHh4eCA0NBSffvopbpTvtFotNBqN2YOIiIioJhb5WU2tVsPLy8v8hWxs4O7uDrVaXa915ObmYsmSJZg2bZpZ++LFi3HffffB0dERv/76K1544QUUFxfjpZdeqnVdcXFxWLRo0c0PhIiIiFqdm9pzNG/evBoPiDZ9nDlz5raL0mg0GDFiBLp27YqFCxeazZs/fz4GDhyI3r17Y+7cuXj11Vfx9ttv17m+mJgYFBYWGh8XLly47RqJiIioZbqpPUezZ8/GpEmT6uzTqVMnqFQqXLp0yay9oqIC+fn5UKlUdS5fVFSEqKgouLi44Pvvv4etrW2d/cPCwrBkyRJotVrI5fIa+8jl8lrnEREREZm6qXDk6ekJT0/PG/YLDw9HQUEBkpKS0LdvXwDArl27YDAYEBYWVutyGo0GkZGRkMvl+PHHH2Fvb3/D1zp27Bjc3NwYfoiIiKhBWOSYoy5duiAqKgpTp07F2rVrUV5ejhdffBFjx441nqmWlZWFIUOGYOPGjQgNDYVGo8HQoUNx9epVfP7552YHTnt6ekImk+Gnn35CTk4O7rzzTtjb22PHjh1YunQpXnnlFUsMg4iIiFohi13n6IsvvsCLL76IIUOGQCqV4pFHHsF7771nnF9eXo6UlBRcvXoVAHDkyBHjmWyBgYFm60pLS0OHDh1ga2uL1atXY9asWRBCIDAwEO+88w6mTp1qqWEQERFRK2OR6xw1dbzOERERUfPTrK9zRERERNRcMRwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZEJhiMiIiIiEwxHRERERCYYjoiIiIhMMBwRERERmWA4IiIiIjJhsXCUn5+P8ePHQ6FQwNXVFVOmTEFxcXGdywwaNAgSicTs8dxzz5n1ycjIwIgRI+Do6AgvLy/MmTMHFRUVlhoGERERtTI2llrx+PHjkZ2djR07dqC8vByTJ0/GtGnT8OWXX9a53NSpU7F48WLjtKOjo/G5Xq/HiBEjoFKpsG/fPmRnZ2PChAmwtbXF0qVLLTUUIiIiakUkQgjR0CtNTk5G165dcejQIfTr1w8AEB8fj+HDhyMzMxO+vr41Ljdo0CD06tULK1eurHH+tm3b8MADD+DixYvw9vYGAKxduxZz587F5cuXYWdnV6/6NBoNlEolCgsLoVAobn6ARERE1Oga6/vbInuOEhMT4erqagxGABAREQGpVIoDBw7goYceqnXZL774Ap9//jlUKhVGjhyJ+fPnG/ceJSYmokePHsZgBACRkZF4/vnncerUKfTu3bvGdWq1Wmi1WuN0YWEhgMo3mYiIiJqHqu9tC+zXMWORcKRWq+Hl5WX+QjY2cHd3h1qtrnW5J554Av7+/vD19cWJEycwd+5cpKSk4LvvvjOu1zQYATBO17XeuLg4LFq0qFq7n59fvcdERERETUNeXh6USqXF1n9T4WjevHl466236uyTnJx8y8VMmzbN+LxHjx7w8fHBkCFDcO7cOQQEBNzyemNiYhAdHW2cLigogL+/PzIyMiz65jY1Go0Gfn5+uHDhQqv6OZHj5rhbA46b424NCgsL0b59e7i7u1v0dW4qHM2ePRuTJk2qs0+nTp2gUqlw6dIls/aKigrk5+dDpVLV+/XCwsIAAKmpqQgICIBKpcLBgwfN+uTk5ABAneuVy+WQy+XV2pVKZav6j6qKQqHguFsRjrt14bhbl9Y6bqnUslciuqlw5OnpCU9Pzxv2Cw8PR0FBAZKSktC3b18AwK5du2AwGIyBpz6OHTsGAPDx8TGu94033sClS5eMP9vt2LEDCoUCXbt2vZmhEBEREdXIItGrS5cuiIqKwtSpU3Hw4EH88ccfePHFFzF27FjjmWpZWVkIDg427gk6d+4clixZgqSkJKSnp+PHH3/EhAkTcM8996Bnz54AgKFDh6Jr16546qmncPz4cWzfvh2vvfYapk+fXuOeISIiIqKbZbH9Ul988QWCg4MxZMgQDB8+HHfddRc+/PBD4/zy8nKkpKTg6tWrAAA7Ozvs3LkTQ4cORXBwMGbPno1HHnkEP/30k3EZmUyGn3/+GTKZDOHh4XjyyScxYcIEs+si1YdcLkdsbGyrC1QcN8fdGnDcHHdrwHFbdtwWuc4RERERUXPFe6sRERERmWA4IiIiIjLBcERERERkguGIiIiIyATDEREREZGJFhmO3njjDQwYMACOjo5wdXWtsU9GRgZGjBgBR0dHeHl5Yc6cOaioqKhzvfn5+Rg/fjwUCgVcXV0xZcoUFBcXW2AEDWPPnj2QSCQ1Pg4dOlTrcoMGDarW/7nnnmvEym9fhw4dqo3hzTffrHOZsrIyTJ8+HW3atIGzszMeeeQR4xXYm4P09HRMmTIFHTt2hIODAwICAhAbGwudTlfncs1xe69evRodOnSAvb09wsLCql05/3qbN29GcHAw7O3t0aNHD2zdurWRKm0YcXFx6N+/P1xcXODl5YXRo0cjJSWlzmXWr19fbbva29s3UsUNY+HChdXGEBwcXOcyzX1bAzV/fkkkEkyfPr3G/s11W//+++8YOXIkfH19IZFIsGXLFrP5QggsWLAAPj4+cHBwQEREBM6ePXvD9d7s50NNWmQ40ul0eOyxx/D888/XOF+v12PEiBHQ6XTYt28fNmzYgPXr12PBggV1rnf8+PE4deoUduzYgZ9//hm///672f3gmpoBAwYgOzvb7PHMM8+gY8eO6NevX53LTp061Wy5ZcuWNVLVDWfx4sVmY5gxY0ad/WfNmoWffvoJmzdvxm+//YaLFy/i4YcfbqRqb9+ZM2dgMBjwwQcf4NSpU1ixYgXWrl2Lf/3rXzdctjlt702bNiE6OhqxsbE4cuQIQkJCEBkZWe2WRVX27duHcePGYcqUKTh69ChGjx6N0aNH4+TJk41c+a377bffMH36dOzfvx87duxAeXk5hg4dipKSkjqXUygUZtv1/PnzjVRxw+nWrZvZGPbu3Vtr35awrQHg0KFDZmPesWMHAOCxxx6rdZnmuK1LSkoQEhKC1atX1zh/2bJleO+997B27VocOHAATk5OiIyMRFlZWa3rvNnPh1qJFmzdunVCqVRWa9+6dauQSqVCrVYb29asWSMUCoXQarU1ruv06dMCgDh06JCxbdu2bUIikYisrKwGr90SdDqd8PT0FIsXL66z37333itefvnlxinKQvz9/cWKFSvq3b+goEDY2tqKzZs3G9uSk5MFAJGYmGiBChvHsmXLRMeOHevs09y2d2hoqJg+fbpxWq/XC19fXxEXF1dj/8cff1yMGDHCrC0sLEw8++yzFq3Tki5duiQAiN9++63WPrV9/jUnsbGxIiQkpN79W+K2FkKIl19+WQQEBAiDwVDj/JawrQGI77//3jhtMBiESqUSb7/9trGtoKBAyOVy8dVXX9W6npv9fKhNi9xzdCOJiYno0aMHvL29jW2RkZHQaDQ4depUrcu4urqa7XGJiIiAVCrFgQMHLF5zQ/jxxx+Rl5eHyZMn37DvF198AQ8PD3Tv3h0xMTHGK5k3J2+++SbatGmD3r174+23367zZ9OkpCSUl5cjIiLC2BYcHIz27dsjMTGxMcq1iMLCwnrdvbq5bG+dToekpCSz7SSVShEREVHrdkpMTDTrD1T+e2/u2xXADbdtcXEx/P394efnh1GjRtX6+daUnT17Fr6+vujUqRPGjx+PjIyMWvu2xG2t0+nw+eef4+mnn4ZEIqm1X0vY1qbS0tKgVqvNtqdSqURYWFit2/NWPh9qc1M3nm0p1Gq1WTACYJxWq9W1LlN1s9sqNjY2cHd3r3WZpuaTTz5BZGQk2rVrV2e/J554Av7+/vD19cWJEycwd+5cpKSk4LvvvmukSm/fSy+9hD59+sDd3R379u1DTEwMsrOz8c4779TYX61Ww87Ortoxat7e3s1m+14vNTUVq1atwvLly+vs15y2d25uLvR6fY3/fs+cOVPjMrX9e2+u29VgMGDmzJkYOHAgunfvXmu/oKAgfPrpp+jZsycKCwuxfPlyDBgwAKdOnbrhZ0BTERYWhvXr1yMoKAjZ2dlYtGgR7r77bpw8eRIuLi7V+re0bQ0AW7ZsQUFBASZNmlRrn5awra9Xtc1uZnveyudDbZpNOJo3bx7eeuutOvskJyff8GC9luBW3ovMzExs374d33zzzQ3Xb3ocVY8ePeDj44MhQ4bg3LlzCAgIuPXCb9PNjDs6OtrY1rNnT9jZ2eHZZ59FXFxcs7sX0a1s76ysLERFReGxxx7D1KlT61y2qW5vqtn06dNx8uTJOo+9AYDw8HCEh4cbpwcMGIAuXbrggw8+wJIlSyxdZoMYNmyY8XnPnj0RFhYGf39/fPPNN5gyZYoVK2s8n3zyCYYNG2a8aXtNWsK2bmqaTTiaPXt2nckZADp16lSvdalUqmpHr1edlaRSqWpd5voDuioqKpCfn1/rMpZyK+/FunXr0KZNGzz44IM3/XphYWEAKvdEWPPL8nb+GwgLC0NFRQXS09MRFBRUbb5KpYJOp0NBQYHZ3qOcnJxG377Xu9lxX7x4EYMHD8aAAQPMbvZcX01le9fEw8MDMpms2lmEdW0nlUp1U/2bshdffNF4MsjN7hGwtbVF7969kZqaaqHqLM/V1RV33HFHrWNoSdsaAM6fP4+dO3fe9F7clrCtq7ZZTk4OfHx8jO05OTno1atXjcvcyudDrW7qCKVm5kYHZOfk5BjbPvjgA6FQKERZWVmN66o6IPvw4cPGtu3btzeLA7INBoPo2LGjmD179i0tv3fvXgFAHD9+vIErazyff/65kEqlIj8/v8b5VQdk/+9//zO2nTlzptkdkJ2ZmSk6d+4sxo4dKyoqKm5pHU19e4eGhooXX3zROK3X60Xbtm3rPCD7gQceMGsLDw9vVgfpGgwGMX36dOHr6yv++uuvW1pHRUWFCAoKErNmzWrg6hpPUVGRcHNzE++++26N81vCtjYVGxsrVCqVKC8vv6nlmuO2Ri0HZC9fvtzYVlhYWK8Dsm/m86HWem6qdzNx/vx5cfToUbFo0SLh7Owsjh49Ko4ePSqKioqEEJX/4XTv3l0MHTpUHDt2TMTHxwtPT08RExNjXMeBAwdEUFCQyMzMNLZFRUWJ3r17iwMHDoi9e/eKzp07i3HjxjX6+G7Wzp07BQCRnJxcbV5mZqYICgoSBw4cEEIIkZqaKhYvXiwOHz4s0tLSxA8//CA6deok7rnnnsYu+5bt27dPrFixQhw7dkycO3dOfP7558LT01NMmDDB2Of6cQshxHPPPSfat28vdu3aJQ4fPizCw8NFeHi4NYZwSzIzM0VgYKAYMmSIyMzMFNnZ2caHaZ/mvr2//vprIZfLxfr168Xp06fFtGnThKurq/Hs06eeekrMmzfP2P+PP/4QNjY2Yvny5SI5OVnExsYKW1tb8eeff1prCDft+eefF0qlUuzZs8dsu169etXY5/pxL1q0SGzfvl2cO3dOJCUlibFjxwp7e3tx6tQpawzhlsyePVvs2bNHpKWliT/++ENEREQIDw8PcenSJSFEy9zWVfR6vWjfvr2YO3dutXktZVsXFRUZv58BiHfeeUccPXpUnD9/XgghxJtvvilcXV3FDz/8IE6cOCFGjRolOnbsKEpLS43ruO+++8SqVauM0zf6fKivFhmOJk6cKABUe+zevdvYJz09XQwbNkw4ODgIDw8PMXv2bLN0vnv3bgFApKWlGdvy8vLEuHHjhLOzs1AoFGLy5MnGwNWUjRs3TgwYMKDGeWlpaWbvTUZGhrjnnnuEu7u7kMvlIjAwUMyZM0cUFhY2YsW3JykpSYSFhQmlUins7e1Fly5dxNKlS832Cl4/biGEKC0tFS+88IJwc3MTjo6O4qGHHjILFk3dunXravzv3nQHcUvZ3qtWrRLt27cXdnZ2IjQ0VOzfv98479577xUTJ0406//NN9+IO+64Q9jZ2Ylu3bqJX375pZErvj21bdd169YZ+1w/7pkzZxrfI29vbzF8+HBx5MiRxi/+NowZM0b4+PgIOzs70bZtWzFmzBiRmppqnN8St3WV7du3CwAiJSWl2ryWsq2rvmevf1SNzWAwiPnz5wtvb28hl8vFkCFDqr0f/v7+IjY21qytrs+H+pIIIcTN/RBHRERE1HK1yuscEREREdWG4YiIiIjIBMMRERERkQmGIyIiIiITDEdEREREJhiOiIiIiEwwHBERERGZYDgiIiIiMsFwRERERGSC4YiIiIjIBMMRERERkYn/B8Xgo/Wafqy+AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma rede neural corresponde a vários desses neurônios sigmoides conectados entre si por pesos e com vieses associados em múltiplas camadas. Os neurônios da camada de entrada são correspondentes aos dados de entrada, sem vieses associados, e com pesos individuais de cada neurônio dessa camada para cada um da próxima camada. As camadas de saída são correspondentes aos neurônios finais com as saídas geradas a partir da rede neural. As camadas \"escondidas\" são aquelas que não são nem de entrada e nem de saída, sendo que os neurônios de uma camada escondida específica recebem entradas com a aplicação dos pesos de cada neurônio da camada anterior e, junto do viés associado a cada neurônio individual da camada atual, geram saídas por meio da função sigmoide. Cada saída também é conectada aos neurônios da camada seguinte através da aplicação dos pesos da camada atual. Em nossos exemplos, usaremos o conjunto de dados MNIST, que consiste em imagens de dígitos e seus algarismos correspondentes como alvos. Cada imagem é $28\\times 28$, então podemos usar $28\\cdot 28=784$ neurônios de entrada, ou seja, um vetor de $784$ elementos, que representa a intensidade de cinza de cada pixel. O alvo é um vetor de $10$ elementos com apenas uma posição igual a $1$ e todas as outras iguais a $0$, de modo que o índice (começando de $0$) de uma posição igual a $1$ corresponde ao algarismo correspondente àquele representado na imagem ($y(x)=(0,0,1,0,0,0,0,0,0,0)^T$, para alguma entrada $x$, corresponde ao dígito alvo $2$, por exemplo).\n",
        "\n",
        "Apresentamos agora um exemplo de função de custo ou erro que usaremos:\n",
        "\\begin{align*}\n",
        "C(w,b)=\\frac{1}{2n}\\sum_x ||y(x)-a||^2,\n",
        "\\end{align*}\n",
        "em que $w$ é todo peso da rede, $b$ todo viés, $n$ o número de amostras de treinamento, $y$ o alvo da entrada $x$ e $a$ o vetor de saídas da rede a partir de $x$ também, além de $w$ e $b$. Chamamos $C$ de função de custo quadrático ou erro quadrático médio. Como os modelos lineares, o objetivo dos algoritmos de redes neurais é minimizar $C$ e, para isso, também é possível utilizar o método do gradiente descendente. Seja $\\Delta C$ a mudança em $C$. Seja $v=(v_1,v_2,\\dots,v_m)^T$ a entrada da função $C$. Pela regra da cadeia do Cálculo de múltiplas variáveis, sabemos que\n",
        "\\begin{align*}\n",
        "\\Delta C &\\approx \\frac{\\delta C}{\\delta v_1}\\Delta v_1 + \\frac{\\delta C}{\\delta v_2}\\Delta v_2 + \\dots + \\frac{\\delta C}{\\delta v_m}\\Delta v_m\n",
        "&= \\nabla C \\cdot \\Delta v,\n",
        "\\end{align*}\n",
        "em que \\Delta v_i representa uma mudança na direção do vetor $v_i$, para $i\\in {1,\\dots,m}$, $\\nabla C = \\biggl(\\frac{\\delta C}{\\delta v_1},\\dots, \\frac{\\delta C}{\\delta v_m}\\biggr)^T$ e $\\Delta v=(\\Delta v_1, \\dots, \\Delta v_m)^T$. Suponha, similarmente à regressão logística, que escolhemos $\\Delta v = -\\eta\\nabla C$ ($\\eta$ é chamada de taxa de aprendizado). Dessa forma, $\\Delta C\\approx -\\eta\\nabla C\\cdot \\nabla C=-\\eta||\\nabla C||^2$ e, em nossos algoritmos, podemos fazer $v\\gets v'=v-\\eta\\nabla C$. Em termos dos pesos $w_k$ e vieses $b_l$, poderíamos fazer\n",
        "\\begin{align*}\n",
        "w_k&\\gets w_k'=w_k-\\eta\\frac{\\delta C}{\\delta w_k}\\\\\n",
        "b_l&\\gets b_l'=b_l-\\eta\\frac{\\delta C}{\\delta b_l}.\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "ETO4QdAYnNrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. O algoritmo de backpropagation\n",
        "\n",
        "Para computar $\\frac{\\delta C}{\\delta w_k}$ e $\\frac{\\delta C}{\\delta b_l}$ rapidamente, utilizamos o chamado algoritmo de *backpropagation*. Antes de apresentá-lo, devemos introduzir notações úteis."
      ],
      "metadata": {
        "id": "nSatn0uFBPxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1. Notação matricial\n",
        "Seja $w_{jk}^l$ o peso para a conexão do $k$-ésimo neurônio na $(l-1)$-ésima camada para o $j$-ésimo neurônio na $l$-ésima camada. De modo similar, seja $b_j^l$ o viés do $j$-ésimo neurônio na $l$-ésima camada. Por fim, seja $a_j^l$ a *ativação* ou saída do $j$-ésimo neurônio da $l$-ésima camada, dependente da ativação da camada anterior (a camada $l-1$), de modo que\n",
        "\\begin{align*}\n",
        "a_j^l=\\sigma\\biggl(\\sum_k w_{jk}^l a_k^{l-1} + b_j^l \\biggr),\n",
        "\\end{align*}\n",
        "sendo que a soma é sobre todos os neurônios $k$ da $(l-1)$-ésima camada. Na notação matricial, definimos uma matriz $w^l$ para cada camada $l$, em que o elemento na $j$-ésima linha e $k$-ésima coluna corresponde a $w_{jk}^l$. Do mesmo modo, definimos o vetor $b^l$, em que o elemento da $j$-ésima linha corresponde a $b_j^l$ e, por fim, definimos o vetor $a^l$, em que o elemento da $j$-ésima linha corresponde a $a_j^l$. Assim, podemos escrever agora $a^l$ como\n",
        "\\begin{align*}\n",
        "a^l=\\sigma(w^l a^{l-1} + b^l).\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "VoM01WizCRQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2. As equações de backpropagation e suas provas\n",
        "Para calcular $\\frac{\\delta C}{\\delta w_{jk}^l}$ e $\\frac{\\delta C}{\\delta b_j^l}$, precisamos definir $\\delta_j^l$, o erro no $j$-ésimo neurônio da $l$-ésima camada. Para isso, considere que, com uma pequena mudança $\\Delta z_k^l$ na entrada do neurônio associado, temos agora, em vez de $\\sigma(z_k^l)$ como saída, $\\sigma(z_k^l+\\Delta z_k^l)$. Essa mudança propaga sobre a rede neural na etapa de alimentação dela com as entradas (etapa chamada de *feedforward*), e o custo, no geral, muda em $\\frac{\\delta C}{\\delta z_j^l}\\Delta z_j^l$. Nesse contexto, consideramos o primeiro operando da multiplicação como uma medida de erro e denotamos ela por $\\delta_j^l=\\frac{\\delta C}{\\delta z_j^l}$.\n",
        "\n",
        "**Primeira equação — erro na camada de saída, $\\delta^L$:**\n",
        "\n",
        "$\\delta_j^L$ pode ser denotado por\n",
        "\\begin{align*}\n",
        "\\delta_j^L=\\frac{\\delta C}{\\delta a_j^L}\\sigma'(z_j^L).\n",
        "\\end{align*}\n",
        "Em notação matricial, podemos denotá-la por\n",
        "\\begin{align*}\n",
        "\\delta^L=\\nabla_a C \\odot \\sigma'(z^L),\n",
        "\\end{align*}\n",
        "sendo que $\\nabla_a C=\\biggl(\\frac{\\delta C}{\\delta a_1^L},\\dots, \\frac{\\delta C}{\\delta a_m^L} \\biggr)^T$, para todos os $m$ neurônios na $L$-ésima camada (que é a camada de saída). Utilizamos a multiplicação de Hadamard $\\odot$ porque a equação vista antes corresponde à multiplicação de cada par de elementos de $\\nabla_a C$ e $\\sigma'(z^L)$. Para a função $C$ escolhida, sabemos que $\\nabla_a C=(a^L-y)$, ou seja,\n",
        "\\begin{align*}\n",
        "\\delta^L=(a^L-y)\\odot \\sigma'(z^L).\n",
        "\\end{align*}\n",
        "\n",
        "***Prova:*** aplicando a regra da cadeia em $\\delta_j^L=\\frac{\\delta C}{\\delta z_j^L}$, obtemos\n",
        "\\begin{align*}\n",
        "\\delta_j^L=\\sum_k \\frac{\\delta C}{\\delta a_k^L} \\frac{\\delta a_k^L}{\\delta z_j^L},\n",
        "\\end{align*}\n",
        "sendo que a soma é sobre todos os neurônios $k$ da camada de saída. A ativação $a_k^L$ depende apenas de $z_j^L$ para o $j$-ésimo neurônio quando $k=j$, ou seja, $\\frac{\\delta a_k^L}{\\delta z_j^L}$ é zero quando $k\\neq j$. Assim,\n",
        "\\begin{align*}\n",
        "\\delta_j^L&=\\frac{\\delta C}{\\delta a_j^L}\\frac{\\delta a_j^L}{\\delta z_j^L}\\\\\n",
        "&=\\frac{\\delta C}{\\delta a_j^L} \\sigma'(z_j^L),\n",
        "\\end{align*}\n",
        "sabendo que $a_j^L=\\sigma(z_j^L)$ e $\\sigma'(z_j^L)=\\frac{\\delta a_j^L}{\\delta z_j^L}$.\n",
        "\n",
        "**Segunda equação — erro na camada $l$, $\\delta^l$, em termos do erro na próxima camada, $\\delta^{l+1}$:**\n",
        "\n",
        "Sabendo o erro $\\delta^{l+1}$, podemos descobrir o erro na camada anterior, $\\delta^l$:\n",
        "\\begin{align*}\n",
        "\\delta^l=((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l).\n",
        "\\end{align*}\n",
        "\n",
        "***Prova:*** é possível diferenciar $\\delta_j^l=\\frac{\\delta C}{\\delta z_j^l}$ em função de $z_k^{l+1}$ para os neurônios $k$ na camada seguinte. Lembre-se que $C$ tem relação com os resultados da camada de saída, então essa diferenciação leva a um caminho útil. Assim,\n",
        "\\begin{align*}\n",
        "\\delta_j^l&=\\frac{\\delta C}{\\delta z_j^l}\\\\\n",
        "&=\\sum_k \\frac{\\delta C}{\\delta z_k^{l+1}}\\frac{\\delta z_k^{l+1}}{\\delta z_k^l}\\\\\n",
        "&=\\sum_k \\delta_k^{l+1} \\frac{\\delta z_k^{l+1}}{\\delta z_k^l}.\n",
        "\\end{align*}\n",
        "Note que $z_k^{l+1}=\\sum_j w_{kj}^{l+1}a_j^l+b_k^{l+1}=\\sum_j w_{kj}^{l+1}\\sigma(z_j^l)+b_k^{l+1}$. Diferenciando esse valor em termos de $z_j^l$, obtemos\n",
        "\\begin{align*}\n",
        "\\frac{\\delta z_k^{l+1}}{\\delta z_j^l}=w_{kj}^{l+1}\\sigma'(z_j^l).\n",
        "\\end{align*}\n",
        "Dessa forma,\n",
        "\\begin{align*}\n",
        "\\delta_j^l = \\sum_k w_{kj}^{l+1}\\delta_k^{l+1}\\sigma'(z_j^l),\n",
        "\\end{align*}\n",
        "que são os componentes de $\\delta^l$ da equação original.\n",
        "\n",
        "**Terceira equação — taxa de mudança de $C$ em função de qualquer viés $b_j^l$:**\n",
        "\\begin{align*}\n",
        "\\frac{\\delta C}{\\delta b_j^l}=\\delta_j^l.\n",
        "\\end{align*}\n",
        "\n",
        "***Prova:*** diferenciando em termos de $z_k^l$, obtemos\n",
        "\\begin{align*}\n",
        "\\frac{\\delta C}{\\delta b_j^l}&=\\sum_k \\frac{\\delta C}{\\delta z_k^l} \\frac{\\delta z_k^l}{\\delta b_j^l}\\\\\n",
        "&=\\frac{\\delta C}{\\delta z_j^l} \\frac{\\delta z_j^l}{\\delta b_j^l}\\\\\n",
        "&=\\frac{\\delta C}{\\delta z_j^l}\\\\\n",
        "&=\\delta_j^l.\n",
        "\\end{align*}\n",
        "A segunda linha se deve ao fato de que a saída $z_k^l$ depende apenas de $b_j^l$ para o $j$-ésimo neurônio quando $k=j$, ou seja, $\\frac{\\delta z_k^l}{\\delta b_j^l}$ é zero quando $k\\neq j$, de modo similar ao desenvolvimento da primeira equação. Além disso, a terceira linha se deve ao fato de que $\\frac{\\delta_j^l}{\\delta b_j^l}=\\frac{\\delta}{\\delta b_j^l}(\\sum_k w_{jk}^l a_k^{l-1} + b_j^l)=1$.\n",
        "\n",
        "**Quarta equação — taxa de mudança de $C$ em função de qualquer peso $w_{jk}^l$:**\n",
        "\\begin{align*}\n",
        "\\frac{\\delta C}{\\delta w_{jk}^l}=a_k^{l-1}\\delta_j^l.\n",
        "\\end{align*}\n",
        "\n",
        "***Prova:*** Diferenciando em termos de $z_i^l$, obtemos\n",
        "\\begin{align*}\n",
        "\\frac{\\delta C}{\\delta w_{jk}^l}&=\\sum_i \\frac{\\delta C}{\\delta z_i^l} \\frac{\\delta z_i^l}{\\delta w_{jk}^l}\\\\\n",
        "&=\\frac{\\delta C}{\\delta z_j^l} \\frac{\\delta z_j^l}{\\delta w_{jk}^l}\\\\\n",
        "&=\\frac{\\delta C}{\\delta z_j^l}a_k^{l-1}\\\\\n",
        "&=\\delta_j^l a_k^{l-1}.\n",
        "\\end{align*}\n",
        "A segunda linha se deve ao fato de que a saída $z_i^l$ depende apenas de $w_{jk}^l$ (depende de todos os $k$ pesos $w_{jk}^l$ para $j$ fixo, mas, nesse caso, $k$ também está fixo) para o $i$-ésimo neurônio quando $i=j$, ou seja, $\\frac{\\delta z_i^l}{\\delta w_{jk}^l}$ é zero quando $i\\neq j$, de modo similar ao desenvolvimento da primeira e segunda equações. Além disso, a terceira linha se deve ao fato de que $\\frac{\\delta z_j^l}{\\delta w_{jk}^l}=\\frac{\\delta}{\\delta w_{jk}^l}(\\sum_i w_{ji}^l a_i^{l-1} + b_j^l)=a_k^{l-1}$.\n",
        "\n",
        "Aqui acaba a apresentação das equações e suas provas. Vimos que os termos necessários para o método do gradiente descendente podem ser facilmente computados por meio de algumas equações simples. Note que, para obter $\\frac{\\delta C}{\\delta w_{jk}^l}$ e $\\frac{\\delta C}{\\delta b_j^l}$, precisamos de $\\delta_j^l$ e, para obter $\\delta_j^l$, precisamos de $\\delta^{l+1},\\dots,\\delta^L$. Assim, o algoritmo de backpropagation funciona da seguinte forma: com a entrada $x$, consiga $a^1$ para a camada de entrada; para $l=2,\\dots, L$, compute $z^l=w^l a^{l-1}+b^l$ e $a^l=\\sigma(z^l)$ (etapa de feedforward); compute $\\delta^L=\\nabla_a C \\odot \\sigma'(z^L)$; para $l=L-1,\\dots, 2$, compute $\\delta^l=((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^L)$ (etapa de backpropagation); compute $\\frac{\\delta C}{\\delta w_{jk}^l}=a_k^{l-1}\\delta_j^l$ e $\\frac{\\delta C}{\\delta b_j^l}=\\delta_j^l$. Ademais, é importante notar que o algoritmo pode demorar de executar quando o dataset é grande. Um método para deixar a computação mais eficiente é usar o chamado gradiente descendente estocástico. Nesse método, selecionamos apenas $m$ amostras $X_1,\\dots,X_m$ do conjunto de dados em cada época, chamados de *mini-batch*, e calculamos\n",
        "\\begin{align*}\n",
        "\\frac{\\sum_{j=1}^m \\nabla C_{X_j}}{m} \\approx \\frac{\\sum_x \\nabla C_x}{n}=\\nabla C.\n",
        "\\end{align*}\n",
        "$\\biggl($Note também que $C$ tem a forma $\\frac{1}{n}\\sum_x C_x$, sendo que $C_x=\\frac{||y(x)-a||^2}{2}.\\biggr)$ Dessa forma, podemos fazer\n",
        "\\begin{align*}\n",
        "w_j&\\gets w_k'=w_k-\\frac{\\eta}{m}\\sum_j \\frac{\\delta C_{X_j}}{\\delta w_k}\\\\\n",
        "b_l&\\gets b_l'=b_l-\\frac{\\eta}{m}\\sum_j \\frac{\\delta C_{X_j}}{\\delta b_l}.\n",
        "\\end{align*}\n",
        "\n",
        "A implementação de uma rede neural para o dataset do MNIST com uma camada escondida segue abaixo."
      ],
      "metadata": {
        "id": "UDw73UcaHTqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Código do livro Learning from data (https://github.com/mnielsen/neural-networks-and-deep-learning)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "network.py\n",
        "~~~~~~~~~~\n",
        "\n",
        "A module to implement the stochastic gradient descent learning\n",
        "algorithm for a feedforward neural network.  Gradients are calculated\n",
        "using backpropagation.  Note that I have focused on making the code\n",
        "simple, easily readable, and easily modifiable.  It is not optimized,\n",
        "and omits many desirable features.\n",
        "\"\"\"\n",
        "\n",
        "class Network(object):\n",
        "\n",
        "    def __init__(self, sizes):\n",
        "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
        "        respective layers of the network.  For example, if the list\n",
        "        was [2, 3, 1] then it would be a three-layer network, with the\n",
        "        first layer containing 2 neurons, the second layer 3 neurons,\n",
        "        and the third layer 1 neuron.  The biases and weights for the\n",
        "        network are initialized randomly, using a Gaussian\n",
        "        distribution with mean 0, and variance 1.  Note that the first\n",
        "        layer is assumed to be an input layer, and by convention we\n",
        "        won't set any biases for those neurons, since biases are only\n",
        "        ever used in computing the outputs from later layers.\"\"\"\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            test_data=None):\n",
        "        \"\"\"Train the neural network using mini-batch stochastic\n",
        "        gradient descent.  The ``training_data`` is a list of tuples\n",
        "        ``(x, y)`` representing the training inputs and the desired\n",
        "        outputs.  The other non-optional parameters are\n",
        "        self-explanatory.  If ``test_data`` is provided then the\n",
        "        network will be evaluated against the test data after each\n",
        "        epoch, and partial progress printed out.  This is useful for\n",
        "        tracking progress, but slows things down substantially.\"\"\"\n",
        "        if test_data: n_test = len(test_data)\n",
        "        n = len(training_data)\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [\n",
        "                training_data[k:k+mini_batch_size]\n",
        "                for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch\", j, \":\", self.evaluate(test_data), \"/\", n_test)\n",
        "            else:\n",
        "                print(\"Epoch\", j, \"complete\")\n",
        "\n",
        "    def update_mini_batch(self, mini_batch, eta):\n",
        "        \"\"\"Update the network's weights and biases by applying\n",
        "        gradient descent using backpropagation to a single mini batch.\n",
        "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
        "        is the learning rate.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(mini_batch))*nw\n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb\n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
        "        gradient for the cost function C_x.  ``nabla_b`` and\n",
        "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
        "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * \\\n",
        "            sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "        # Note that the variable l in the loop below is used a little\n",
        "        # differently to the notation in Chapter 2 of the book.  Here,\n",
        "        # l = 1 means the last layer of neurons, l = 2 is the\n",
        "        # second-last layer, and so on.  It's a renumbering of the\n",
        "        # scheme in the book, used here to take advantage of the fact\n",
        "        # that Python can use negative indices in lists.\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        \"\"\"Return the number of test inputs for which the neural\n",
        "        network outputs the correct result. Note that the neural\n",
        "        network's output is assumed to be the index of whichever\n",
        "        neuron in the final layer has the highest activation.\"\"\"\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
        "                        for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "        \\partial a for the output activations.\"\"\"\n",
        "        return (output_activations-y)\n",
        "\n",
        "#### Miscellaneous functions\n",
        "def sigmoid(z):\n",
        "    \"\"\"The sigmoid function.\"\"\"\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "\"\"\"\n",
        "mnist_loader\n",
        "~~~~~~~~~~~~\n",
        "\n",
        "A library to load the MNIST image data.  For details of the data\n",
        "structures that are returned, see the doc strings for ``load_data``\n",
        "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
        "function usually called by our neural network code.\n",
        "\"\"\"\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
        "    the validation data, and the test data.\n",
        "\n",
        "    The ``training_data`` is returned as a tuple with two entries.\n",
        "    The first entry contains the actual training images.  This is a\n",
        "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
        "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "    pixels in a single MNIST image.\n",
        "\n",
        "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "    containing 50,000 entries.  Those entries are just the digit\n",
        "    values (0...9) for the corresponding images contained in the first\n",
        "    entry of the tuple.\n",
        "\n",
        "    The ``validation_data`` and ``test_data`` are similar, except\n",
        "    each contains only 10,000 images.\n",
        "\n",
        "    This is a nice data format, but for use in neural networks it's\n",
        "    helpful to modify the format of the ``training_data`` a little.\n",
        "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
        "    below.\n",
        "    \"\"\"\n",
        "    f = gzip.open(datasets_path + 'mnist.pkl.gz', 'rb')\n",
        "    u = pickle._Unpickler( f )\n",
        "    u.encoding = 'latin1'\n",
        "    training_data, validation_data, test_data = u.load()\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def load_data_wrapper():\n",
        "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
        "    test_data)``. Based on ``load_data``, but the format is more\n",
        "    convenient for use in our implementation of neural networks.\n",
        "\n",
        "    In particular, ``training_data`` is a list containing 50,000\n",
        "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
        "    containing the input image.  ``y`` is a 10-dimensional\n",
        "    numpy.ndarray representing the unit vector corresponding to the\n",
        "    correct digit for ``x``.\n",
        "\n",
        "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
        "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
        "    numpy.ndarry containing the input image, and ``y`` is the\n",
        "    corresponding classification, i.e., the digit values (integers)\n",
        "    corresponding to ``x``.\n",
        "\n",
        "    Obviously, this means we're using slightly different formats for\n",
        "    the training data and the validation / test data.  These formats\n",
        "    turn out to be the most convenient for use in our neural network\n",
        "    code.\"\"\"\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = list(zip(training_inputs, training_results))\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = list(zip(test_inputs, te_d[1]))\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    (0...9) into a corresponding desired output from the neural\n",
        "    network.\"\"\"\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "random.seed(42)\n",
        "training_data, validation_data, test_data = load_data_wrapper()\n",
        "epochs = 10\n",
        "mini_batch_size = 10\n",
        "eta = 0.1\n",
        "net = Network([784, 30, 10])\n",
        "net.SGD(training_data, epochs, mini_batch_size, eta, test_data=test_data)\n",
        "\n",
        "print(\"Precisão obtida:\", \"{precisao:.2f}%\".format(precisao=net.evaluate(test_data) / len(test_data) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpM-3lZlr5pK",
        "outputId": "60c28149-3165-4316-dbea-76a50d6c46d3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 : 4083 / 10000\n",
            "Epoch 1 : 5150 / 10000\n",
            "Epoch 2 : 5702 / 10000\n",
            "Epoch 3 : 6214 / 10000\n",
            "Epoch 4 : 6749 / 10000\n",
            "Epoch 5 : 6959 / 10000\n",
            "Epoch 6 : 7036 / 10000\n",
            "Epoch 7 : 7076 / 10000\n",
            "Epoch 8 : 7118 / 10000\n",
            "Epoch 9 : 7166 / 10000\n",
            "Precisão obtida: 71.66%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Regularização\n",
        "\n",
        "Para melhorar o treinamento e prevenir overfitting (realizar a regularização), visto anteriormente, é possível implementar duas técnicas simples para ajudar as redes neurais a aprender os padrões verdadeiramente gerais e relevantes contidos em datasets. Uma dessas técnicas é o *dropout*, que consiste em aleatoriamente e temporariamente cortar alguns dos neurônios das camadas escondidas, passar a entrada em feedforward nessa rede, realizar o backpropagation nessa mesma rede, atualizar os pesos e vieses, restaurar a arquitetura de rede original e realizar o mesmo processo em outro mini-batch. Isso é como se estivéssemos treinando redes diferentes e fazendo a média dos resultados, de modo que prevenimos o overfitting diferente que pode ocorrer de cada rede através da captura de padrões gerais que todas as redes costumam obter. A outra técnica, chamada de *data augmentation*, consiste em artificialmente expandir o dataset quando não é possível realizar uma expansão de fato. Fazemos isso por meio da seleção e alteração pequena das características dos dados no conjunto. No exemplo do MNIST, poderíamos selecionar as imagens, realizar pequenos deslocamentos nelas e incluir essas novas instâncias no conjunto de dados, já que dígitos ainda são reconhecíveis com pequenos deslocamentos realizados neles. Essa técnica também ajuda muito as redes neurais a captarem os padrões relevantes em datasets.\n",
        "\n",
        "Uma implementação com PyTorch de uma rede que utiliza essas técnicas segue abaixo."
      ],
      "metadata": {
        "id": "dMvA1fHlEnfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vide expand_mnist.py do mesmo repositório citado do livro\n",
        "\n",
        "expanded = True\n",
        "\n",
        "if not expanded:\n",
        "  expanded_training_pairs = []\n",
        "  j = 0 # counter\n",
        "  for x, y in zip(training_data[0], training_data[1]):\n",
        "    expanded_training_pairs.append((x, y))\n",
        "    image = np.reshape(x, (-1, 28))\n",
        "    j += 1\n",
        "    if j % 1000 == 0: print(\"Expanding image number\", j)\n",
        "    # iterate over data telling us the details of how to\n",
        "    # do the displacement\n",
        "    for d, axis, index_position, index in [\n",
        "          (1,  0, \"first\", 0),\n",
        "          (-1, 0, \"first\", 27),\n",
        "          (1,  1, \"last\",  0),\n",
        "          (-1, 1, \"last\",  27)]:\n",
        "        new_img = np.roll(image, d, axis)\n",
        "        if index_position == \"first\":\n",
        "          new_img[index, :] = np.zeros(28)\n",
        "        else:\n",
        "          new_img[:, index] = np.zeros(28)\n",
        "        expanded_training_pairs.append((np.reshape(new_img, 784), y))\n",
        "  random.shuffle(expanded_training_pairs)\n",
        "  expanded_training_data = [list(d) for d in zip(*expanded_training_pairs)]\n",
        "  print(\"Saving expanded data. This may take a few minutes.\")\n",
        "  f = gzip.open(datasets_path + \"mnist_expanded.pkl.gz\", \"w\")\n",
        "  pickle.dump((expanded_training_data, validation_data, test_data), f)\n",
        "  f.close()\n",
        "\n",
        "f = gzip.open(datasets_path + \"mnist_expanded.pkl.gz\", 'rb')\n",
        "u = pickle._Unpickler( f )\n",
        "u.encoding = 'latin1'\n",
        "training_data, validation_data, test_data = u.load()\n",
        "f.close()\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = map(tensor, (np.array(training_data[0]), np.array(training_data[1]),\n",
        "                                                  np.array(validation_data[0]), np.array(validation_data[1])))\n",
        "\n",
        "num_rows, _ = x_train.shape\n",
        "\n",
        "model_linear = nn.Sequential(nn.Linear(784,30), nn.Sigmoid(), nn.Dropout(0.1), nn.Linear(30,10), nn.Sigmoid(), nn.Linear(10,10), nn.Sigmoid())\n",
        "opt = optim.SGD(model_linear.parameters(), lr=eta)\n",
        "loss_func = nn.functional.mse_loss\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i in range((num_rows-1)//mini_batch_size + 1):\n",
        "        start_i = i * mini_batch_size\n",
        "        end_i = start_i + mini_batch_size\n",
        "        xb = x_train[start_i:end_i]\n",
        "        yb = y_train[start_i:end_i]\n",
        "        pred = model_linear(xb)\n",
        "        yb = nn.functional.one_hot(yb, num_classes=10).to(torch.float32)\n",
        "        loss = loss_func(pred, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "def accuracy(out, y_batch):\n",
        "    return (torch.argmax(out, dim=1)==y_batch).float().mean()\n",
        "\n",
        "print(\"Precisão obtida: {precisao:.2f}%\".format(precisao=accuracy(model_linear(x_valid), y_valid).item() * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYM252ryKR8v",
        "outputId": "036e46ef-c0c4-471f-e909-4d41b30fdb77"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precisão obtida: 91.44%\n"
          ]
        }
      ]
    }
  ]
}